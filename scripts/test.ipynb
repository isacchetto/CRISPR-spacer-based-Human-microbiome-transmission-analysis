{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readline\n",
    "\n",
    "def input_with_prefill(prompt, text):\n",
    "    def hook():\n",
    "        readline.insert_text(text)\n",
    "        readline.redisplay()\n",
    "    readline.set_pre_input_hook(hook)\n",
    "    result = input(prompt)\n",
    "    readline.set_pre_input_hook()\n",
    "    return result\n",
    "\n",
    "\n",
    "print(input_with_prefill('Enter something: ', 'default text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if shutil.which('pilercr'):\n",
    "    print('python3 is installed')\n",
    "else:\n",
    "    print('python3 is not installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_file=os.path.join(output_dir, os.path.relpath(mag.rsplit('.', 2 if args.decompress else 1)[0]+\".\"+tool, input_dir))\n",
    "\n",
    "tool=\"minced\"\n",
    "input_dir = '/shares/CIBIO-Storage/CM/scratch/users/isacco.cenacchi/Tirocinio/samples/MAGs_mini'\n",
    "output_dir = \"/shares/CIBIO-Storage/CM/scratch/users/isacco.cenacchi/Tirocinio/out/MAGs_mini_CRISPRDetect3_20241001161255\"\n",
    "\n",
    "# output_file = os.path.join(input_dir.removesuffix(os.path.basename(input_dir)), os.path.basename(args.out))\n",
    "os.path.basename(input_dir)+'_'+tool+'_parsed.tsv'\n",
    "os.path.basename(output_dir)+'_parsed.tsv'\n",
    "os.path.join(output_dir, os.path.basename(input_dir)+'_'+tool+'_parsed.tsv')\n",
    "# input_dir.removesuffix(os.path.basename(input_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = [\"..A..................................CC.\", \".....................................---\", \"GG............-......................--.\"]\n",
    "reference = \"AAGTTTCCGTCCCCTTTCGGGGAATCATTTAGAAAAT--A\"\n",
    "line=\"   145662      33    93.9      33  TAGTATTTAA    ...............T................    AGAAAACCGCATAAGGACCGATACCACATTACA\"\n",
    "stringa = \".--.-...............T................\"\n",
    "count = stringa[:stringa.find(\".\")].count(\"-\")\n",
    "seqs = line.split()\n",
    "start=int(seqs[0]) - seqs[5][:seqs[5].find(\".\")].count(\"-\")\n",
    "def develop_repeats(repeats, reference):\n",
    "    developed_repeats = []\n",
    "    for repeat in repeats:\n",
    "        repeat = list(repeat)\n",
    "        for i in range(len(reference)):\n",
    "            if repeat[i] == '.': \n",
    "                repeat[i] = reference[i]\n",
    "        repeat = ''.join(repeat)\n",
    "        repeat = repeat.replace('-', '')\n",
    "        developed_repeats.append(repeat)\n",
    "    return developed_repeats\n",
    "\n",
    "develop_repeats(repeats, reference)\n",
    "start\n",
    "# print(develop_repeats(repeats, reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from run_CRISPRtools import *\n",
    "\n",
    "# Path to the uploaded file\n",
    "gff_file_path = '/Users/isaccocenacchi/Desktop/Tirocinio/tests/M1023330751/M1023330751_real_cpu64_NOdirection.CRISPRDetect3.gff'\n",
    "# gff_file_path = '/Users/isaccocenacchi/Desktop/Tirocinio/tests/M1023330751/prova.gff'\n",
    "\n",
    "# Parse the GFF file\n",
    "crisprs = parse_CRISPRDetect3(gff_file_path)\n",
    "\n",
    "for crispr in crisprs:\n",
    "    print(crispr)\n",
    "    print(crispr.sequence())\n",
    "    print([len(rep) for rep in crispr.repeats])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root_dir = '/Users/isaccocenacchi/Desktop/Tirocinio/out/MAGs_mini_CRISPRtools'\n",
    "\n",
    "parsed_files = [os.path.join(dirpath,filename)\n",
    "                for dirpath, _, filenames in os.walk(output_root_dir)\n",
    "                for filename in filenames\n",
    "                if filename.endswith('_parsed.tsv')\n",
    "            ]\n",
    "\n",
    "print('\\n'.join(parsed_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGGING\n",
    "\n",
    "%reset -f\n",
    "import logging\n",
    "# logging.basicConfig(format='[%(asctime)s] %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S', level='INFO')\n",
    "\n",
    "logger=logging.getLogger(__name__)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "formatter=logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "handler=logging.StreamHandler()\n",
    "handler.setLevel('INFO')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "handler=logging.FileHandler('test.log')\n",
    "handler.setLevel('INFO')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "print(f'name: {logger.name}\\nhandlers: {logger.handlers}\\nparent: {logger.parent}')\n",
    "logger.info('prova')\n",
    "\n",
    "for i in range(1,4):\n",
    "    handler=logging.FileHandler(f'tool{i}.log')\n",
    "    handler.setLevel('INFO')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    logger.info(f'tool{i}')\n",
    "    print(f'name: {logger.name}\\nhandlers: {logger.handlers}\\nparent: {logger.parent}')\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "logger.info('fine')\n",
    "print(f'name: {logger.name}\\nhandlers: {logger.handlers}')\n",
    "# logger_tool.removeHandler(logging.FileHandler('test.log'))\n",
    "\n",
    "\n",
    "# logging.shutdown()\n",
    "\n",
    "\n",
    "import logging\n",
    "import types\n",
    "\n",
    "def log_newline(self, how_many_lines=1):\n",
    "    # Switch handler, output a blank line\n",
    "    self.removeHandler(self.console_handler)\n",
    "    self.addHandler(self.blank_handler)\n",
    "    for i in range(how_many_lines):\n",
    "        self.info('')\n",
    "\n",
    "    # Switch back\n",
    "    self.removeHandler(self.blank_handler)\n",
    "    self.addHandler(self.console_handler)\n",
    "\n",
    "def create_logger():\n",
    "    # Create a handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.DEBUG)\n",
    "    console_handler.setFormatter(logging.Formatter(fmt=\"%(name)s %(levelname)-8s: %(message)s\"))\n",
    "\n",
    "    # Create a \"blank line\" handler\n",
    "    blank_handler = logging.StreamHandler()\n",
    "    blank_handler.setLevel(logging.DEBUG)\n",
    "    blank_handler.setFormatter(logging.Formatter(fmt=''))\n",
    "\n",
    "    # Create a logger, with the previously-defined handler\n",
    "    logger = logging.getLogger('logging_test')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # Save some data and add a method to logger object\n",
    "    logger.console_handler = console_handler\n",
    "    logger.blank_handler = blank_handler\n",
    "    logger.newline = types.MethodType(log_newline, logger)\n",
    "\n",
    "    return logger\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = create_logger()\n",
    "    logger.info('Start reading database')\n",
    "    logger.info('Updating records ...')\n",
    "    logger.newline()\n",
    "    logger.info('Finish updating records')\n",
    "\n",
    "\n",
    "logging.getLoggerClass().newline = log_newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f string indentation\n",
    "import logging\n",
    "import types\n",
    "def x(a):\n",
    "    return (f'f_name: {a}\\n'\n",
    "                f'contig: {a}\\n'\n",
    "                f'start: {a}\\n'\n",
    "                f'end: {a}\\n'\n",
    "                f'repeats: {a}\\n'\n",
    "                f'spacers: {a}\\n'\n",
    "                )\n",
    "    \n",
    "file = 'prova.tsv'\n",
    "args = types.SimpleNamespace()\n",
    "args.cas_database = True\n",
    "\n",
    "logging.error(f'Check the column names in the parsed file {file} '\n",
    "                f'(MAG, Contig, Start, End, Spacers, Repeats, ToolCodename'\n",
    "                f'{\", Cas_0-1000, Cas_1000-10000, Cas_>100000, Cas_overlayed\" if args.cas_database else \"\"}), '\n",
    "                f'and secure that file is a TSV file')\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from telegram import Bot\n",
    "from logging import Handler\n",
    "import logging\n",
    "import asyncio\n",
    "import requests\n",
    "import os\n",
    "\n",
    "TELEGRAM_BOT_TOKEN=os.environ.get('TELEGRAM_BOT_TOKEN')\n",
    "TELEGRAM_CHAT_ID=os.environ.get('TELEGRAM_CHAT_ID')\n",
    "\n",
    "if not TELEGRAM_BOT_TOKEN and not TELEGRAM_CHAT_ID:\n",
    "     raise Exception('Please set the environment variables TELEGRAM_TOKEN and TELEGRAM_CHAT_ID')\n",
    "\n",
    "\n",
    "bot = Bot(token=TELEGRAM_BOT_TOKEN)\n",
    "\n",
    "async def send_message(text, chat_id):\n",
    "    async with bot:\n",
    "        await bot.send_message(text=text, chat_id=chat_id)\n",
    "\n",
    "class BotHandler(Handler):\n",
    "    def emit(self, record):\n",
    "        log_entry = self.format(record)\n",
    "        asyncio.run(send_message(log_entry, TELEGRAM_CHAT_ID))\n",
    "\n",
    "class RequestsHandler(Handler):\n",
    "\tdef emit(self, record):\n",
    "\t\tlog_entry = self.format(record)\n",
    "\t\tpayload = {\n",
    "\t\t\t'chat_id': TELEGRAM_CHAT_ID,\n",
    "\t\t\t'text': log_entry,\n",
    "\t\t\t'parse_mode': 'HTML'\n",
    "\t\t}\n",
    "\t\treturn requests.post(f'https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage', data=payload).content\n",
    "\n",
    "logging.basicConfig(format='[%(asctime)s] %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S', level='INFO')\n",
    "logger=logging.getLogger(__name__)\n",
    "\n",
    "notification_handler=RequestsHandler()\n",
    "notification_handler.setLevel('ERROR')\n",
    "# log_formatter=logging.Formatter(\"<i>[%(asctime)s] %(levelname)s:</i><pre>\\n%(message)s</pre>\", datefmt='%Y-%m-%d %H:%M:%S')\n",
    "log_formatter=logging.Formatter('[%(asctime)s] %(levelname)s:\\n%(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "notification_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(notification_handler)\n",
    " \n",
    "logger.error('We have a problem')\n",
    "logger.info('This is an info message')\n",
    "\n",
    "logger.removeHandler(notification_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import os\n",
    "a=os.environ.items()\n",
    "\n",
    "for key, value in os.environ.items():\n",
    "    print(f'{key}: {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats=['cccc', \"s\", 'tttt']\n",
    "spacers=['aaasf', 'sdf', 'sdf']\n",
    "spacers1=['','','']\n",
    "\n",
    "# all(repeat !='' for repeat in repeats)\n",
    "all(repeats)\n",
    "# all(spacers)\n",
    "# [repeat !='' for repeat in repeats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {'MAG': str, 'Contig': str, 'Start': int, 'End': int, 'ToolCodename': str, 'Cas_0-1000': int, 'Cas_1000-10000': int, 'Cas_>100000': int, 'Cas_overlayed': int}\n",
    "\n",
    "columns_groupby = list(columns.keys())\n",
    "columns_groupby.remove('ToolCodename')\n",
    "\n",
    "columns_groupby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/shares/CIBIO-Storage/CM/scratch/users/isacco.cenacchi/Tirocinio/out/MAGs_mini_CRISPRtools/test.tsv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "input_file='/shares/CIBIO-Storage/CM/scratch/users/isacco.cenacchi/Tirocinio/sample/MAGs_mini/Aug16/0/M12345676.fna'\n",
    "\n",
    "parsed_file = '/shares/CIBIO-Storage/CM/scratch/users/isacco.cenacchi/Tirocinio/out/MAGs_mini_CRISPRtools/MAGs_mini_CRISPRDetect3/MAGs_mini_minced_parsed.tsv'\n",
    "output_root_dir = '/shares/CIBIO-Storage/CM/scratch/users/isacco.cenacchi/Tirocinio/out/MAGs_mini_CRISPRtools'\n",
    "os.path.relpath(parsed_file, output_root_dir.rsplit('/', 1)[0])\n",
    "os.path.relpath(input_file, output_root_dir)\n",
    "\n",
    "pathlib.Path(output_root_dir).joinpath('test.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logomaker as lm\n",
    "import tempfile\n",
    "import shutil\n",
    "import subprocess\n",
    "from Bio import SeqIO\n",
    "import tempfile\n",
    "import subprocess\n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Data un'insieme di sequenze (allineate) calcola la matrice delle informazioni\n",
    "def make_info_df(seqs):\n",
    "    # Create a list of Counters, one for each position in the alignment\n",
    "    counters = [Counter() for i in range(len(seqs[0]))]\n",
    "    for i in range(len(counters)):\n",
    "        for seq in seqs:\n",
    "            if i < len(seq):\n",
    "                counters[i][seq[i]] +=1\n",
    "    # Create a matrix of frequencies\n",
    "    freqs = pd.DataFrame(counters).fillna(0)\n",
    "    # Create a matrix of probabilities\n",
    "    probs = freqs.divide(freqs.sum(axis=1), axis=0).fillna(1.0/freqs.shape[1])\n",
    "    # Check that the probabilities sum to 1.0\n",
    "    if not all(probs.sum(axis=1).apply(lambda x: math.isclose(x, 1.0))):\n",
    "        raise ValueError\n",
    "    # Transform the matrix of probabilities into a matrix of information\n",
    "    info = logomaker.transform_matrix(probs, from_type='probability',\n",
    "        to_type='information')\n",
    "    # Normalize the matrix of information\n",
    "    # (re-become probabilities)\n",
    "    info = info.divide(np.log2(info.shape[1]))\n",
    "    return info\n",
    "\n",
    "\n",
    "def DR_Similarity(repeats):\n",
    "    repeats = repeats.split(',')\n",
    "\n",
    "    # Crea il file FASTA con le sequenze\n",
    "    with tempfile.NamedTemporaryFile(\"w\", suffix=f\".fasta\", delete=True, delete_on_close = False) as input_file, tempfile.NamedTemporaryFile(\"wb\", suffix=f\".aln\", delete=True, delete_on_close = False) as output_file:\n",
    "        for i, seq in enumerate(repeats):\n",
    "            input_file.write(f\">Sequence_{i+1}\\n{seq}\\n\")\n",
    "        input_file.close()\n",
    "\n",
    "        # Esegui il comando Muscle o Mafft \n",
    "        if len(repeats) < 100:\n",
    "            subprocess.run([\"muscle\", \"-align\", input_file.name, \"-output\", output_file.name], \n",
    "                    stdout=subprocess.DEVNULL, \n",
    "                    stderr=subprocess.DEVNULL, \n",
    "                    check=True)\n",
    "        else:\n",
    "            subprocess.run([\"mafft\", input_file.name], \n",
    "                        stdout=output_file, \n",
    "                        stderr=subprocess.DEVNULL, \n",
    "                        check=True)\n",
    "            output_file.close()\n",
    "\n",
    "        # copia il file temporaneo in un file permanente 'sequences.fasta' con python shutil\n",
    "        # shutil.copy(input_file.name, \"sequences.fasta\")\n",
    "        # shutil.copy(output_file.name, \"alignment.aln\")\n",
    "        \n",
    "        # Leggi l'allineamento\n",
    "        seqs = [str(rec.seq).strip().upper() for rec in SeqIO.parse(output_file.name, \"fasta\")]\n",
    "\n",
    "    # create probability matrix (from information matrix)\n",
    "    alignment_df = make_info_df(seqs)\n",
    "\n",
    "    # return the sum of the information content of each position (Site Entropy)\n",
    "    return alignment_df.sum(axis=1).mean()\n",
    "\n",
    "# La tua stringa originale\n",
    "sequenze1 = \"GTTGTGGTTTGGTGATGAAAGAGGATATAACGCAAC,GTTGTGGTTTGGTGATGAAAGAGGATATAACGCAAC,GTTGTGGTTTGGTGATGAAAGAGGATATAACGCAAC,GTTGTGGTTTGGTGATGAAAGAGGATATAACGCAAC,GTTGTGGTTTGGTGATGAAAGAGGATATAACGCAAC,GTTGTGGTTTGGTGATGAAAGAGGATATAACGCAAC,GTTGTGGTTTGGTGATGAAAGAGGATATAACGCAAC,GTTGTGGTTTGGTGATGAAAGAGAATATAACGCAAC,GTTGTGGTTTGGTGATGAAAGAGGATATAACGCAAC,GTTGTGGTTTGGTGATGAAAGAGGATATAACGCAAC,GTTGTGGTTTGGTGATGAAAGAAGATATAGCGCAAC\"\n",
    "sequenze2 = \"GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTATCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTATCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTAATT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTATCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTAATTC,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GCCGATTGTTCTAATTGTACCTTTATGGAATTGAAAT,AGGCTGAGTGTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTATCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT\"\n",
    "sequenze3 = \"GCGAGATTGCCACGTCGCTTCGCTCCTCGCAATGACAAGTCGGGAGCGTTTGCCATTTTGACGGCGGTTTGTACAGATTACAAGTGCGTCGCCGGGCCCGGC,GCGAGATTGCCACGTCGCTTTGCTCCTCGCAATGACAAGTCGGGGGCGTTTGCCATTTTGACGGCGGTTTGTACAGATTACAAGTGCGTCGCCGGGCCCGGC,GCGAGATTGCCACGTCGCTTTGCTCCTCGCAATGACAAGTCGGGGGCGTTTGCCATTTTGACGGCGGTTTGTACAGATTACAAGTGCGTCGCCGGGCCCGGC\"\n",
    "sequenze4 = 'GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,GTTCTAATTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT,TTGTACCTTTATGGAATTGAAAT'\n",
    "sequenze5 = 'GCGCAGCGGATGCGGCGGC,CCGCGGCGGACTTGGCGTC,CAGCGGCCTTCTCGGCGGC,TCTCGGCTTCCTCGGCAGC,CCTCGGCGGCGTCGGCAGC,CCTTTCCGGCCTCCTCAGC,CCTTTGCGGCAGCCTCGGC,CCTTCTGGGCAGCCTCGGC,CAGCCTGGGCAGCCTCAGC,CCTTCTTGGCAGCCTCGCT,CCTTCTGCGCAGCCTCGGC,TCTCCTGCGCAGCCTTGGC,CCTTCTGTGCAGCCTCGAC'\n",
    "sequenze6 = 'AGATAAGTCCCCTAGAGGGGTTAATTGTTATGAAT,GTTTCCGTCCCCAGAAGGGGTTAACTATTATGAAT,GTTTCCGCCCCCCAACAGGGGTTAACTATTATGAAT,TTTTCCGTCCCCCCCAACAGGGATTAACTATTATGAAT,GTTTCCATCCCCAACAGGGGTTAACCATCATGAAT,GTTTCCGTCCCCAACAGGGGTTACTTATTACGAAT,GTTTCCGTCCCCAACAGGGGTTACTTATTATGAAT,GTTTCCGTCCCCAACAGGGGTTAACTATTATGAAT,GTTTCCGTCCCCTATAGGGGTTGACTATTATGAAT,GTTTCCGTCCCCTATAGGGGTTGACTATTATAAAT,GTTTCCGTCCCCTATAGGGGTTAACCATTATGAAT,AGATAAGTCCCCTAGAGGGGTTAATTGTTATGAAT,GTTTCCGTCCCCAGAAGGGGTTAACTATTATGAAT,GTTTCCGCCCCCCAACAGGGGTTAACTATTATGAAT,TTTTCCGTCCCCCCCAACAGGGATTAACTATTATGAAT,GTTTCCATCCCCAACAGGGGTTAACCATCATGAAT,GTTTCCGTCCCCAACAGGGGTTACTTATTACGAAT,GTTTCCGTCCCCAACAGGGGTTACTTATTATGAAT,GTTTCCGTCCCCAACAGGGGTTAACTATTATGAAT,GTTTCCGTCCCCTATAGGGGTTGACTATTATGAAT,GTTTCCGTCCCCTATAGGGGTTGACTATTATAAAT,GTTTCCGTCCCCTATAGGGGTTAACCATTATGAAT,AGGGGTTAATTGTTATGAAT,AGGGGTTAACTATTATGAAT,AGGGGTTAACTATTATGAAT,AGGGATTAACTATTATGAAT,AGGGGTTAACCATCATGAAT,AGGGGTTACTTATTACGAAT,AGGGGTTACTTATTATGAAT,AGGGGTTAACTATTATGAAT,AGGGGTTGACTATTATGAAT,AGGGGTTGACTATTATAAAT,AGGGGTTAACCATTATGAAT,GTTTCCATCCCCAACAGGGGTT,GTTTCCGTCCCCAACAGGGGTT,GTTTCCGTCCCCAACAGGGGTT,GTTTCCGTCCCCAACAGGGGTT,GTTTCCGTCCCCTATAGGGGTT,GTTTCCGTCCCCTATAGGGGTT,GTTTCCGTCCCCTATAGGGGTT,GTTTCCATCCCCAACAGGGGTTA,GTTTCCGTCCCCAACAGGGGTTA,GTTTCCGTCCCCAACAGGGGTTA,GTTTCCGTCCCCAACAGGGGTTA,GTTTCCGTCCCCTATAGGGGTTG,GTTTCCGTCCCCTATAGGGGTTG,GTTTCCGTCCCCTATAGGGGTTA'\n",
    "sequenze7 = 'GTCGGCTCTGTGGGTC,GTCGGCTCTGTGGGTC,GTCGGCTCTGTGGGTC'\n",
    "sequenze_list = sequenze2.split(',')\n",
    "\n",
    "# Crea il file FASTA con le sequenze o eliminalo se esiste già\n",
    "with tempfile.NamedTemporaryFile(\"w\", suffix=f\".fasta\", delete=True, delete_on_close = False) as input_file, tempfile.NamedTemporaryFile(\"wb\", suffix=f\".aln\", delete=True, delete_on_close = False) as output_file:\n",
    "    for i, seq in enumerate(sequenze_list):\n",
    "        input_file.write(f\">Sequence_{i+1}\\n{seq}\\n\")\n",
    "    input_file.close()\n",
    "\n",
    "    # Esegui il comando Muscle o Mafft \n",
    "    if len(sequenze_list) < 100:\n",
    "        subprocess.run([\"muscle\", \"-align\", input_file.name, \"-output\", output_file.name], \n",
    "                   stdout=subprocess.DEVNULL, \n",
    "                   stderr=subprocess.DEVNULL, \n",
    "                   check=True)\n",
    "    else:\n",
    "        subprocess.run([\"mafft\", input_file.name], \n",
    "                       stdout=output_file, \n",
    "                       stderr=subprocess.DEVNULL, \n",
    "                       check=True)\n",
    "        output_file.close()\n",
    "\n",
    "    # copia il file temporaneo in un file permanente 'sequences.fasta' con python shutil\n",
    "    # shutil.copy(input_file.name, \"sequences.fasta\")\n",
    "    # shutil.copy(output_file.name, \"alignment.aln\")\n",
    "    \n",
    "    # Leggi l'allineamento\n",
    "    seqs = [str(rec.seq).strip().upper() for rec in SeqIO.parse(output_file.name, \"fasta\")]\n",
    "    print(\"Seq alignment:\")\n",
    "    print(\"\\n\".join(seqs))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "# create counts matrix\n",
    "# alignment_df = lm.alignment_to_matrix(sequences=seqs, to_type='information', characters_to_ignore='.-X')\n",
    "alignment_df = make_info_df(seqs)\n",
    "print(\"Alignment probability matrix:\")\n",
    "print(alignment_df)\n",
    "print('\\n')\n",
    "\n",
    "# Calcola la media delle informazioni di ogni posizione\n",
    "similarity=alignment_df.sum(axis=1).mean()\n",
    "print(f\"Similarity (with gaps): {similarity}\")\n",
    "\n",
    "alignment_df = alignment_df.drop(columns=['X', '-', '.'], errors='ignore')\n",
    "\n",
    "# Calcola la media delle informazioni di ogni posizione\n",
    "similarity=alignment_df.sum(axis=1).mean()\n",
    "print(f\"Similarity (without gaps): {similarity}\")\n",
    "\n",
    "# Crea il Motif Logo\n",
    "logo = lm.Logo(alignment_df)\n",
    "\n",
    "# style using Logo methods\n",
    "logo.style_spines(visible=False)\n",
    "logo.style_spines(spines=['left', 'bottom'], visible=True)\n",
    "logo.style_xticks(fmt='%d', anchor=0)\n",
    "\n",
    "# style using Axes methods\n",
    "# logo.ax.set_ylabel('probability')\n",
    "logo.ax.xaxis.set_ticks_position('none')\n",
    "logo.ax.xaxis.set_tick_params(pad=-1)\n",
    "# logo.ax.set_yticks([0, .5, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARICO IL FILE DI ALIGNMENT\n",
    "\n",
    "df_alignment = pd.read_csv('/home/isacco.cenacchi/data/Tirocinio/out/MAGs_CRISPRtools/align_spacers_CRISPRDetect3_cpu.tsv', \n",
    "                           sep='\\t', index_col=False, \n",
    "                           names=['qseqid', 'sseqid', 'pident', 'nident', 'qlen', 'slen', 'length', 'mismatch', 'gapopen', 'qseq', 'sseq', 'qstart', 'qend', 'sstart', 'send', 'evalue', 'sstrand', 'bitscore'],\n",
    "                           dtype={'qseqid':str, 'sseqid':str, 'pident':float, 'nident':int, 'qlen':int, 'slen':int, 'length':int, 'mismatch':int, 'gapopen':int, 'qseq':str, 'sseq':str, 'qstart':int, 'qend':int, 'sstart':int, 'send':int, 'evalue':float, 'sstrand':str, 'bitscore':float})\n",
    "\n",
    "# print(df_alignment.head())\n",
    "# print(len(df_alignment))\n",
    "\n",
    "# Filtro i risultati di blastn per avere solo le corrispondenze perfette\n",
    "def filter_matches(blast_result, max_n_diff=3): # input blastn result dataframe\n",
    "    near_perfect_matches = blast_result[\n",
    "        blast_result['qlen']-blast_result['nident'] <= max_n_diff]\n",
    "    full_len_matches = near_perfect_matches[\n",
    "        near_perfect_matches['length']==near_perfect_matches['qlen']]\n",
    "    return full_len_matches\n",
    "\n",
    "df_alignment_filtered = filter_matches(df_alignment)\n",
    "\n",
    "# Apro qseqid per riottenere MAG, Contig, Start e End\n",
    "df_alignment_filtered['MAG'] = df_alignment_filtered['qseqid'].apply(lambda x: x.split('-')[0])\n",
    "df_alignment_filtered['Contig'] = df_alignment_filtered['qseqid'].apply(lambda x: x.split('-')[1])\n",
    "df_alignment_filtered['Start'] = df_alignment_filtered['qseqid'].apply(lambda x: int(x.split('-')[2]))\n",
    "df_alignment_filtered['End'] = df_alignment_filtered['qseqid'].apply(lambda x: int(x.split('-')[3]))\n",
    "\n",
    "# Funzione per verificare l'uniformità e mantenere il valore se è uguale\n",
    "def verifica_uniformita(x):\n",
    "    if x.nunique() == 1:  # Se ci sono valori unici\n",
    "        return x.iloc[0]  # Mantieni il primo\n",
    "    else:\n",
    "        return None  # Restituisci None se non sono uguali\n",
    "\n",
    "# Raggruppo quelli con lo stesso spacer\n",
    "df_spacer_matched = df_alignment_filtered.groupby('qseqid').agg({\n",
    "    'sseqid': lambda x: ','.join(x), \n",
    "    'MAG': verifica_uniformita, \n",
    "    'Contig': verifica_uniformita, \n",
    "    'Start': verifica_uniformita, \n",
    "    'End': verifica_uniformita})\n",
    "\n",
    "# Aggiungo il conteggio di quanti elementi sono stati raggruppati per ogni spacer\n",
    "df_spacer_matched['num_matches'] = df_alignment_filtered.groupby('qseqid').size()\n",
    "\n",
    "# Raggruppo quelli con lo stesso CRISPR\n",
    "df_crispr_matched = df_spacer_matched.groupby(['MAG', 'Contig', 'Start', 'End'], as_index=False).agg({\n",
    "    'MAG': verifica_uniformita,\n",
    "    'Contig': verifica_uniformita,\n",
    "    'Start': verifica_uniformita,\n",
    "    'End': verifica_uniformita,\n",
    "    'sseqid': lambda x: ','.join(x), \n",
    "    'num_matches': lambda x: ','.join(x.astype(str))\n",
    "    })\n",
    "\n",
    "# Aggiungo il conteggio di quanti elementi sono stati raggruppati per ogni CRISPR\n",
    "df_crispr_matched['num_spacer_matched'] = df_crispr_matched['num_matches'].apply(lambda x: len(x.split(',')))\n",
    "\n",
    "\n",
    "# Seleziona solo le colonne chiave e quella da aggiungere dal secondo dataframe\n",
    "df_temp = df_crispr_matched[['MAG', 'Contig', 'Start', 'End', 'num_spacer_matched']]\n",
    "\n",
    "# Filtra il primo dataframe per la condizione su ToolCodename\n",
    "mask = df_exploded['ToolCodename'] == 'CRISPRDetect3_cpu'\n",
    "\n",
    "# Effettua il merge solo sul subset filtrato\n",
    "df_exploded_filtered = df_exploded[mask].merge(\n",
    "    df_temp, \n",
    "    on=['MAG', 'Contig', 'Start', 'End'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Aggiorna il dataframe originale con i nuovi valori\n",
    "df_exploded.loc[mask, 'nSP_matched'] = df_exploded_filtered['num_spacer_matched']\n",
    "\n",
    "# Riempi i valori NaN con 0\n",
    "df_exploded['nSP_matched'] = df_exploded['nSP_matched'].fillna(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CRISPR-transmission",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
